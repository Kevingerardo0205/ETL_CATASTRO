# -*- coding: utf-8 -*-
"""SRI_catastro_N.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X4csq4i2pYHdFo6jac_2K_-RKMWE10by
"""

# Instalar librerías necesarias
!pip install --upgrade gspread google-auth-oauthlib google-cloud-bigquery pandas beautifulsoup4 requests gspread-dataframe

# Importar las librerías
import pandas as pd
import requests
from bs4 import BeautifulSoup
import os
import zipfile
import time
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import auth, files
from google.auth import default
import gspread
from google.colab import data_table
from google.cloud import bigquery
import warnings

# Configuraciones para mejor visualización y manejo
warnings.filterwarnings('ignore')
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (14, 7)
data_table.enable_dataframe_formatter()

print("Entorno preparado.")

# URL de la página de datasets del SRI
sri_url = 'https://www.sri.gob.ec/datasets'

print(f"Accediendo a la página: {sri_url}")

try:
    # Realizar la petición GET a la página
    response = requests.get(sri_url)
    response.raise_for_status() # Lanza un error si la petición no fue exitosa (ej. 404)

    # Parsear el contenido HTML con BeautifulSoup
    soup = BeautifulSoup(response.content, 'html.parser')

    # Encontrar todos los enlaces (<a>) cuyo href contiene el patrón de los archivos RUC
    base_url = "https://descargas.sri.gob.ec"
    download_links = []

    # Buscamos enlaces que específicamente contengan 'SRI_RUC_' en su URL
    for link in soup.find_all('a', href=True):
        if 'download/datosAbiertos/SRI_RUC_' in link['href']:
            full_link = link['href']
            # Asegurarse de que el link sea absoluto
            if not full_link.startswith('http'):
                full_link = base_url + full_link
            download_links.append(full_link)

    # Eliminar duplicados si los hubiera
    download_links = sorted(list(set(download_links)))

    if download_links:
        print(f"\n¡Éxito! Se encontraron {len(download_links)} enlaces de descarga para el Catastro RUC.")
        for l in download_links:
            print(f" - {l}")
    else:
        print("\nNo se encontraron enlaces de descarga. La estructura de la página del SRI pudo haber cambiado.")

except requests.exceptions.RequestException as e:
    print(f"Error al acceder a la página del SRI: {e}")

# PASO 3 REINVENTADO: DESCARGA Y CONSOLIDACIÓN EN PARALELO PARA MÁXIMA VELOCIDAD

# Importar las librerías necesarias para el procesamiento en paralelo
import concurrent.futures
import threading # Para obtener el ID del hilo y ver el paralelismo en acción

if 'download_links' in locals() and download_links:

    # --- Función que define la "unidad de trabajo" ---
    # Cada hilo (worker) ejecutará esta función para un enlace de descarga.
    def descargar_y_procesar_provincia(link):
        """
        Descarga un archivo CSV desde un enlace, lo convierte a DataFrame y lo devuelve.
        Maneja errores de forma individual para no detener todo el proceso.
        """
        # Obtenemos el nombre de la provincia para los mensajes de log
        provincia = link.split('_')[-1].replace('.csv', '')
        # Obtenemos el identificador del hilo que está ejecutando esta tarea
        thread_id = threading.get_ident()

        try:
            print(f"[Hilo {thread_id}] Inicia descarga y procesamiento de: {provincia}...")
            # Usamos pandas para leer el CSV directamente desde la URL.
            # Especificamos el separador y la codificación.
            df_provincia = pd.read_csv(link, sep='|', encoding='latin1', low_memory=False)
            print(f"    -> [Hilo {thread_id}] ¡Éxito! {provincia} procesado con {len(df_provincia)} registros.")
            return df_provincia
        except Exception as e:
            print(f"    -> [Hilo {thread_id}] *** ERROR al procesar {provincia}: {e}. Omitiendo este archivo. ***")
            return None # Devolvemos None si hay un error

    # --- Ejecución en Paralelo ---
    print("\n--- Iniciando descarga y consolidación en PARALELO ---")

    # Define cuántos archivos descargar simultáneamente. Un valor entre 8 y 16 es un buen comienzo.
    MAX_WORKERS = 10

    dataframes_list = []

    # ThreadPoolExecutor gestiona un "pool" de hilos trabajadores.
    # El `with` se asegura de que todos los hilos se cierren correctamente.
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # `executor.map` aplica la función `descargar_y_procesar_provincia` a cada `link` en `download_links`.
        # Ejecuta las llamadas en paralelo y devuelve los resultados en el mismo orden.
        results = executor.map(descargar_y_procesar_provincia, download_links)

        # Recolectamos los resultados (los DataFrames) a medida que se completan.
        for df_result in results:
            if df_result is not None: # Solo añadimos a la lista si la descarga fue exitosa
                dataframes_list.append(df_result)

    # --- Consolidación Final ---
    if dataframes_list:
        print("\n--- Consolidación completada ---")
        print(f"Se procesaron exitosamente {len(dataframes_list)} de {len(download_links)} archivos.")

        # Combinar todos los dataframes de la lista en uno solo
        df = pd.concat(dataframes_list, ignore_index=True)

        print(f"Total de registros combinados: {len(df)}")
        print("Mostrando las primeras 5 filas del DataFrame consolidado:")
        display(df.head())
    else:
        print("\nNo se pudo descargar y procesar ningún archivo. El análisis no puede continuar.")
else:
    print("No hay enlaces de descarga para procesar. Ejecute el paso anterior.")



# PASO 4 CORREGIDO Y DEFINITIVO: LIMPIEZA ULTRA-EFICIENTE (MANEJANDO 'S'/'N')

import gc

if 'df' in locals():
    print("--- Iniciando limpieza y transformación Optimizada para RAM (Versión Corregida) ---")

    try:
        del dataframes_list
        gc.collect()
        print("Memoria de la lista de DataFrames liberada.")
    except NameError:
        print("La lista 'dataframes_list' no existe o ya fue eliminada.")

    # 1. Renombrar columnas
    column_mapping = {
        'NUMERO_RUC': 'ruc', 'RAZON_SOCIAL': 'razon_social', 'PROVINCIA_JURISDICCION': 'provincia_jurisdiccion',
        'ESTADO_CONTRIBUYENTE': 'estado_contribuyente', 'CLASE_CONTRIBUYENTE': 'clase_contribuyente',
        'FECHA_INICIO_ACTIVIDADES': 'fecha_inicio_actividades', 'FECHA_ACTUALIZACION': 'fecha_actualizacion',
        'FECHA_SUSPENSION_DEFINITIVA': 'fecha_suspension_definitiva', 'FECHA_REINICIO_ACTIVIDADES': 'fecha_reinicio_actividades',
        'OBLIGADO': 'obligado_contabilidad', 'TIPO_CONTRIBUYENTE': 'tipo_contribuyente',
        'NUMERO_ESTABLECIMIENTO': 'numero_establecimientos', 'NOMBRE_FANTASIA_COMERCIAL': 'nombre_comercial',
        'ESTADO_ESTABLECIMIENTO': 'estado_establecimiento', 'DESCRIPCION_PROVINCIA_EST': 'provincia_establecimiento',
        'DESCRIPCION_CANTON_EST': 'canton_establecimiento', 'DESCRIPCION_PARROQUIA_EST': 'parroquia_establecimiento',
        'CODIGO_CIIU': 'codigo_ciiu', 'ACTIVIDAD_ECONOMICA': 'actividad_economica',
        'AGENTE_RETENCION': 'es_agente_retencion', 'ESPECIAL': 'es_contribuyente_especial'
    }
    existing_columns_to_rename = {k: v for k, v in column_mapping.items() if k in df.columns}
    # Usamos df.rename para evitar el KeyError si una columna no existe
    df.rename(columns={k: v for k, v in existing_columns_to_rename.items() if k in df.columns}, inplace=True)
    print("\n--- Columnas renombradas ---")

    # 2. Optimización de Tipos de Datos (Dtypes) - Lógica Corregida
    print("\n--- Optimizando tipos de datos para reducir uso de RAM ---")
    memoria_antes = df.memory_usage(deep=True).sum() / 1024**2
    print(f"Uso de memoria ANTES de la optimización: {memoria_antes:.2f} MB")

    # Columnas binarias que usan 'S'/'N'
    binary_cols = ['obligado_contabilidad', 'es_agente_retencion', 'es_contribuyente_especial']

    for col in df.columns:
        # PRIMERO, manejamos las columnas binarias de forma especial
        if col in binary_cols:
            print(f"Procesando columna binaria: '{col}'...")
            # SOLUCIÓN: Mapear 'S' a 1. Cualquier otra cosa ('N', nulos, etc.) se convierte en 0.
            df[col] = (df[col].astype(str).str.strip().str.upper() == 'S').astype('int8')

        # LUEGO, manejamos las otras columnas de texto
        elif df[col].dtype == 'object':
            num_valores_unicos = len(df[col].unique())
            num_total = len(df[col])

            # Limpiar primero
            df[col] = df[col].fillna('NO ESPECIFICADO').astype(str).str.strip().str.upper()

            # Convertir a 'category' si es eficiente
            if num_valores_unicos / num_total < 0.5:
                df[col] = df[col].astype('category')

    # Convertir columnas de fecha (esto se mantiene igual)
    date_cols = [col for col in df.columns if 'fecha' in col]
    for col in date_cols:
        df[col] = pd.to_datetime(df[col], errors='coerce')

    memoria_despues = df.memory_usage(deep=True).sum() / 1024**2
    print(f"Uso de memoria DESPUÉS de la optimización: {memoria_despues:.2f} MB")
    print(f"¡Reducción de {(memoria_antes - memoria_despues) / memoria_antes * 100:.2f}% en el uso de memoria!")

    # 3. Verificación final
    print("\n--- Limpieza de alta eficiencia completada ---")
    df_limpio = df

    print("\nInformación del DataFrame optimizado:")
    df_limpio.info(memory_usage='deep')

    print("\nMostrando valores únicos de columnas clave para verificar:")
    for col in binary_cols:
        if col in df_limpio.columns:
            print(f"Valores en '{col}': {df_limpio[col].unique().tolist()}")

    del df
    gc.collect()

else:
    print("El DataFrame 'df' no fue creado. Revise el paso de descarga.")

# PASO 5 REINVENTADO: ANÁLISIS DESCRIPTIVO (VERSIÓN FINAL, A PRUEBA DE ERRORES Y TIPOS DE DATOS)

if 'df_limpio' in locals():
    print("\n--- INICIANDO ANÁLISIS DESCRIPTIVO PROFUNDO DEL CATASTRO RUC ---")

    # --- ANÁLISIS 1: Contribuyentes por Provincia ---
    col_provincia = 'provincia_jurisdiccion'
    if col_provincia in df_limpio.columns:
        print(f"\n[Análisis 1] Creando gráfico para '{col_provincia}'...")
        plt.figure(figsize=(12, 8))
        conteo_provincia = df_limpio[col_provincia].value_counts().nlargest(15).copy()
        sns.barplot(y=conteo_provincia.index, x=conteo_provincia.values, palette='viridis')
        plt.title('Top 15 Provincias por Número de Contribuyentes (Jurisdicción)', fontsize=16)
        plt.xlabel('Cantidad de Contribuyentes')
        plt.ylabel('Provincia de Jurisdicción')
        plt.show()
    else:
        # Intento de solución alternativa: buscar otra columna de provincia
        col_provincia_alt = 'provincia_establecimiento'
        if col_provincia_alt in df_limpio.columns:
            print(f"\n[Análisis 1] '{col_provincia}' no encontrada. Usando alternativa: '{col_provincia_alt}'...")
            plt.figure(figsize=(12, 8))
            conteo_provincia = df_limpio[col_provincia_alt].value_counts().nlargest(15).copy()
            sns.barplot(y=conteo_provincia.index, x=conteo_provincia.values, palette='viridis')
            plt.title('Top 15 Provincias por Número de Contribuyentes (Establecimiento)', fontsize=16)
            plt.xlabel('Cantidad de Contribuyentes')
            plt.ylabel('Provincia del Establecimiento')
            plt.show()
        else:
            print(f"\n[Análisis 1] OMITIDO: No se encontró ni '{col_provincia}' ni '{col_provincia_alt}'.")


    # --- ANÁLISIS 2: Contribuyentes Especiales ---
    col_especial = 'es_contribuyente_especial'
    if col_especial in df_limpio.columns:
        print(f"\n[Análisis 2] Creando gráfico para '{col_especial}'...")
        especial_counts = df_limpio[col_especial].value_counts()

        # DEBUG: Imprimir los valores encontrados para verificar
        print(f"    Valores únicos encontrados en '{col_especial}': {especial_counts.index.tolist()}")

        labels = {1: 'Sí, es Contribuyente Especial', 0: 'No es Contribuyente Especial'}
        # SOLUCIÓN: Convertir el índice a entero estándar de Python antes de buscar
        pie_labels = [labels[int(i)] for i in especial_counts.index if int(i) in labels]

        # VERIFICACIÓN FINAL: Asegurarse de que la longitud coincida antes de graficar
        if len(pie_labels) == len(especial_counts):
            plt.figure(figsize=(8, 8))
            plt.pie(especial_counts, labels=pie_labels, autopct='%1.2f%%', startangle=90,
                    colors=['#ff9999','#66b3ff'])
            plt.title('Proporción de Contribuyentes Especiales', fontsize=16)
            plt.ylabel('')
            plt.show()
        else:
            print(f"    ERROR DE COINCIDENCIA: No se pudieron generar etiquetas para los valores {especial_counts.index.tolist()}. Gráfico omitido.")
    else:
        print(f"\n[Análisis 2] OMITIDO: La columna '{col_especial}' no se encontró en el DataFrame.")

    # --- ANÁLISIS 3: Agentes de Retención ---
    col_retencion = 'es_agente_retencion'
    if col_retencion in df_limpio.columns:
        print(f"\n[Análisis 3] Creando gráfico para '{col_retencion}'...")
        retencion_counts = df_limpio[col_retencion].value_counts()

        # DEBUG: Imprimir los valores encontrados
        print(f"    Valores únicos encontrados en '{col_retencion}': {retencion_counts.index.tolist()}")

        labels = {1: 'Sí, es Agente de Retención', 0: 'No es Agente de Retención'}
        # SOLUCIÓN: Convertir el índice a entero estándar
        pie_labels = [labels[int(i)] for i in retencion_counts.index if int(i) in labels]

        if len(pie_labels) == len(retencion_counts):
            plt.figure(figsize=(8, 8))
            plt.pie(retencion_counts, labels=pie_labels, autopct='%1.1f%%', startangle=90,
                    colors=['#99ff99','#ffcc99'])
            plt.title('Proporción de Agentes de Retención', fontsize=16)
            plt.ylabel('')
            plt.show()
        else:
            print(f"    ERROR DE COINCIDENCIA: No se pudieron generar etiquetas para los valores {retencion_counts.index.tolist()}. Gráfico omitido.")
    else:
        print(f"\n[Análisis 3] OMITIDO: La columna '{col_retencion}' no se encontró en el DataFrame.")

    # --- ANÁLISIS 4: Actividades Económicas ---
    # (Este bloque ya era robusto, no necesita cambios)
    col_actividad = 'actividad_economica'
    if col_actividad in df_limpio.columns:
        print(f"\n[Análisis 4] Creando gráfico para '{col_actividad}'...")
        actividades_validas = df_limpio[~df_limpio[col_actividad].isin(['NO ESPECIFICADO', ''])]

        if not actividades_validas.empty:
            conteo_actividades = actividades_validas[col_actividad].value_counts().nlargest(10)
            plt.figure(figsize=(12, 8))
            sns.barplot(y=conteo_actividades.index, x=conteo_actividades.values, palette='plasma')
            plt.title('Top 10 Actividades Económicas Más Comunes', fontsize=16)
            plt.xlabel('Cantidad de Contribuyentes')
            plt.ylabel('Actividad Económica (CIIU)')
            plt.show()
        else:
             print(f"\n[Análisis 4] OMITIDO: No se encontraron datos válidos en la columna '{col_actividad}' después de filtrar.")
    else:
        print(f"\n[Análisis 4] OMITIDO: La columna '{col_actividad}' no se encontró en el DataFrame.")

else:
    print("El DataFrame 'df_limpio' no existe para analizar. Ejecuta el paso 4 primero.")

# Importar la librería 'files' de Google Colab
from google.colab import files

# Verificar que el DataFrame final exista
if 'df_limpio' in locals():
    # 1. Definir el nombre para el archivo de salida
    nombre_archivo = 'analisis_sri_inscripciones_cierres.csv'

    # 2. Convertir el DataFrame a un archivo CSV en el entorno de Colab
    #    - index=False: para no incluir el índice del DataFrame como una columna en el CSV.
    #    - encoding='utf-8-sig': para asegurar la correcta visualización de tildes y ñ en Excel.
    df_limpio.to_csv(nombre_archivo, index=False, encoding='utf-8-sig', sep='|')

    print(f"Archivo '{nombre_archivo}' listo para ser descargado.")

    # 3. Usar la función de Colab para descargar el archivo a tu computadora
    files.download(nombre_archivo)

else:
    print("El DataFrame 'df_limpio' no se ha creado. Por favor, ejecuta los pasos anteriores primero.")